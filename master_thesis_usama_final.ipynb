{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c75e6-8804-4a2b-adaa-7cbe315cc164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import joblib\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import imgaug.augmenters as iaa\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PyQt5.QtWidgets import QApplication, QLabel, QVBoxLayout, QWidget\n",
    "from PyQt5.QtGui import QImage, QPainter, QFont, QColor, QFontMetrics\n",
    "from PyQt5.QtCore import Qt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# constants\n",
    "VALID_EXTENSIONS=('.jpg', '.jpeg', '.png')\n",
    "TRAIN_DATASET_PATH=\"add path here\"\n",
    "TEST_DATASET_PATH=\"add path here\"\n",
    "DATASET_TAGS=[\"original_images\", \"complete_UAlpha40\", \"original_plus_custom_aug\"]\n",
    "\n",
    "# mediapipe initialization\n",
    "mp_hands=mp.solutions.hands\n",
    "hands=mp_hands.Hands(static_image_mode=True,max_num_hands=1,min_detection_confidence=0.5,min_tracking_confidence=0.5)\n",
    "\n",
    "# feature groups for using in feature groups ablation study\n",
    "FEATURE_GROUPS={\n",
    "    'pairwise distances': list(range(0, 210)),\n",
    "    'joint angles': list(range(210, 225)),\n",
    "    'hand position size': list(range(225, 231)),\n",
    "    'hand orientation': list(range(231, 236)),\n",
    "    'finger spread': list(range(236, 246)),\n",
    "    'finger curvature': list(range(246, 251)),\n",
    "    'length ratios': list(range(251, 261)),\n",
    "    'palm relative': list(range(261, 276))\n",
    "}\n",
    "\n",
    "# for feature vector size ablation study\n",
    "FEATURE_COUNTS=[10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,276]\n",
    "\n",
    "# augmentations\n",
    "augmentations = iaa.Sequential([\n",
    "    iaa.Sometimes(0.8,iaa.Affine(\n",
    "        rotate=(-25,25),\n",
    "        scale=(0.9,1.1),\n",
    "        shear=(-10,10),\n",
    "        translate_percent={\"x\":(-0.1,0.1), \"y\":(-0.1,0.1)}\n",
    "    )),\n",
    "    iaa.Sometimes(0.5,iaa.PerspectiveTransform(scale=(0.01,0.05))),\n",
    "    iaa.Sometimes(0.6,iaa.MultiplyAndAddToBrightness(mul=(0.8, 1.2), add=(-20,20))),\n",
    "    iaa.Sometimes(0.4,iaa.GaussianBlur(sigma=(0,0.5))),\n",
    "])\n",
    "\n",
    "# for creating dynamic paths for model saving and loading\n",
    "def get_model_paths(model_type, n_features, dataset_tag=None,base_dir=\"models\"):\n",
    "    model_dir = os.path.join(base_dir,model_type)\n",
    "    os.makedirs(model_dir,exist_ok=True)\n",
    "    prefix = f\"{dataset_tag + '_' if dataset_tag else ''}{model_type}_k{n_features}\"\n",
    "    \n",
    "    return {\n",
    "        \"model\": os.path.join(model_dir,f\"model_{prefix}.pkl\"),\n",
    "        \"encoder\": os.path.join(model_dir,f\"encoder_{prefix}.pkl\"),\n",
    "        \"scaler\": os.path.join(model_dir,f\"scaler_{prefix}.pkl\"),\n",
    "        \"selector\": os.path.join(model_dir,f\"selector_{prefix}.pkl\"),\n",
    "    }\n",
    "\n",
    "# simple helper function used in compute_features\n",
    "def calculate_angle(a, b, c):\n",
    "    ang = math.degrees(math.atan2(c[1]-b[1],c[0]-b[0]) - math.atan2(a[1]-b[1],a[0]-b[0]))\n",
    "    return ang + 360 if ang < 0 else ang\n",
    "\n",
    "# extract_features and compute_features are separate functions \n",
    "# extract_features uses this helper function on the loaded dataset\n",
    "# live_prediction uses it on live user input\n",
    "def compute_features(landmarks):\n",
    "    # normalization of landmarks to wrist and scale\n",
    "    wrist = landmarks[0]\n",
    "    normalized = [(x - wrist[0], y - wrist[1], z - wrist[2]) for x,y,z in landmarks]\n",
    "    \n",
    "    hand_scale = max(math.sqrt(normalized[9][0]**2 + normalized[9][1]**2 + normalized[9][2]**2), 1e-6)\n",
    "    normalized = [(x/hand_scale, y/hand_scale, z/hand_scale) for x, y, z in normalized]\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # pairwise distances\n",
    "    for i in range(21):\n",
    "        for j in range(i+1, 21):\n",
    "            dist = math.sqrt(\n",
    "                (normalized[i][0] - normalized[j][0])**2 + \n",
    "                (normalized[i][1] - normalized[j][1])**2 + \n",
    "                (normalized[i][2] - normalized[j][2])**2\n",
    "            )\n",
    "            features.append(dist)\n",
    "    \n",
    "    # finger joint angles\n",
    "    finger_joints = [\n",
    "        [(0,1,2), (1,2,3), (2,3,4)],      # thumb\n",
    "        [(0,5,6), (5,6,7), (6,7,8)],      # index\n",
    "        [(0,9,10), (9,10,11), (10,11,12)],# middle\n",
    "        [(0,13,14), (13,14,15), (14,15,16)],# ring\n",
    "        [(0,17,18), (17,18,19), (18,19,20)] # pinky\n",
    "    ]\n",
    "    \n",
    "    for finger in finger_joints:\n",
    "        for a, b, c in finger:\n",
    "            ang = math.degrees(\n",
    "                math.atan2(normalized[c][1] - normalized[b][1], normalized[c][0] - normalized[b][0]) - \n",
    "                math.atan2(normalized[a][1] - normalized[b][1], normalized[a][0] - normalized[b][0])\n",
    "            )\n",
    "            features.append(ang + 360 if ang < 0 else ang)\n",
    "    \n",
    "    # bounding box/hand position size features\n",
    "    x_cords = [p[0] for p in landmarks]\n",
    "    y_cords = [p[1] for p in landmarks]\n",
    "    z_cords = [p[2] for p in landmarks]\n",
    "    \n",
    "    features.extend([\n",
    "        (min(x_cords) + max(x_cords)) / 2,\n",
    "        (min(y_cords) + max(y_cords)) / 2,\n",
    "        (min(z_cords) + max(z_cords)) / 2,\n",
    "        max(x_cords) - min(x_cords),\n",
    "        max(y_cords) - min(y_cords),\n",
    "        max(z_cords) - min(z_cords)\n",
    "    ])\n",
    "    \n",
    "    # hand orientation (normal vector)\n",
    "    hand_normal = np.cross(\n",
    "        np.array(normalized[5]) - np.array(normalized[17]),\n",
    "        np.array(normalized[9]) - np.array(normalized[17])\n",
    "    )\n",
    "    norm_mag = np.linalg.norm(hand_normal)\n",
    "    hand_normal = hand_normal / norm_mag if norm_mag > 1e-6 else np.array([0, 0, 0])\n",
    "    features.extend(hand_normal)\n",
    "    \n",
    "    # pitch and roll\n",
    "    features.extend([\n",
    "        np.arctan2(hand_normal[1], np.sqrt(hand_normal[0]**2 + hand_normal[2]**2)),\n",
    "        np.arctan2(hand_normal[0], hand_normal[2])\n",
    "    ])\n",
    "    \n",
    "    # fingertip spread distances\n",
    "    fingertips = [4, 8, 12, 16, 20]\n",
    "    for i in range(len(fingertips)):\n",
    "        for j in range(i+1, len(fingertips)):\n",
    "            dist = math.sqrt(\n",
    "                (normalized[fingertips[i]][0] - normalized[fingertips[j]][0])**2 + \n",
    "                (normalized[fingertips[i]][1] - normalized[fingertips[j]][1])**2 + \n",
    "                (normalized[fingertips[i]][2] - normalized[fingertips[j]][2])**2\n",
    "            )\n",
    "            features.append(dist)\n",
    "    \n",
    "    # finger curvatures\n",
    "    for base, mid, tip in [(0,2,4), (0,6,8), (0,10,12), (0,14,16), (0,18,20)]:\n",
    "        features.append(calculate_angle(normalized[base], normalized[mid], normalized[tip]))\n",
    "    \n",
    "    # finger length ratios\n",
    "    finger_lengths = {\n",
    "        'thumb': math.sqrt(normalized[4][0]**2 + normalized[4][1]**2 + normalized[4][2]**2),\n",
    "        'index': math.sqrt(normalized[8][0]**2 + normalized[8][1]**2 + normalized[8][2]**2),\n",
    "        'middle': math.sqrt(normalized[12][0]**2 + normalized[12][1]**2 + normalized[12][2]**2),\n",
    "        'ring': math.sqrt(normalized[16][0]**2 + normalized[16][1]**2 + normalized[16][2]**2),\n",
    "        'pinky': math.sqrt(normalized[20][0]**2 + normalized[20][1]**2 + normalized[20][2]**2)\n",
    "    }\n",
    "    \n",
    "    names = ['thumb', 'index', 'middle', 'ring', 'pinky']\n",
    "    for i in range(len(names)):\n",
    "        for j in range(i+1, len(names)):\n",
    "            features.append(finger_lengths[names[i]] / (finger_lengths[names[j]] + 1e-6))\n",
    "    \n",
    "    # fingertip positions relative to palm center\n",
    "    palm_center = np.mean([normalized[i] for i in [0, 5, 9, 13, 17]], axis=0)\n",
    "    for tip in fingertips:\n",
    "        features.extend([\n",
    "            normalized[tip][0] - palm_center[0],\n",
    "            normalized[tip][1] - palm_center[1],\n",
    "            normalized[tip][2] - palm_center[2]\n",
    "        ])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def extract_features(image_path, img_size=(224, 224), augment=False):\n",
    "    img = cv2.imdecode(np.fromfile(str(image_path), dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    if augment:\n",
    "        img = augmentations.augment_image(img)\n",
    "    \n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(img)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            landmarks = [(p.x, p.y, p.z) for p in hand_landmarks.landmark]\n",
    "            return compute_features(landmarks)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# can specify limits on samples per class\n",
    "# debug gives you details on how many images failed etc.\n",
    "def load_dataset(dataset_path, limit_per_class=None, debug=False, augment=False):\n",
    "    X_data, y_data = [], []\n",
    "    class_stats = {}\n",
    "    \n",
    "    for label in os.listdir(dataset_path):\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "        \n",
    "        count_loaded, count_failed = 0, 0\n",
    "        image_paths = [\n",
    "            os.path.join(label_path, img) \n",
    "            for img in os.listdir(label_path) \n",
    "            if img.lower().endswith(VALID_EXTENSIONS)\n",
    "        ]\n",
    "        \n",
    "        if limit_per_class and len(image_paths) > limit_per_class:\n",
    "            np.random.shuffle(image_paths)\n",
    "            image_paths = image_paths[:limit_per_class]\n",
    "        \n",
    "        for img_path in image_paths:\n",
    "            features = extract_features(img_path, augment=False)\n",
    "            if features is not None:\n",
    "                X_data.append(features)\n",
    "                y_data.append(label)\n",
    "                count_loaded += 1\n",
    "            else:\n",
    "                count_failed += 1\n",
    "            # range value can be changed, higher = more augmentations applied\n",
    "            if augment:\n",
    "                for _ in range(4):\n",
    "                    aug_features = extract_features(img_path, augment=True)\n",
    "                    if aug_features is not None:\n",
    "                        X_data.append(aug_features)\n",
    "                        y_data.append(label)\n",
    "                        count_loaded += 1\n",
    "        \n",
    "        class_stats[label] = (count_loaded, count_failed)\n",
    "        if debug:\n",
    "            print(f\"{label}: Loaded {count_loaded}, Failed {count_failed}\")\n",
    "    \n",
    "    if debug:\n",
    "        total = sum(loaded for loaded, _ in class_stats.values())\n",
    "        print(f\"\\nTotal samples: {total}\")\n",
    "        print(f\"Classes: {len(class_stats)}\")\n",
    "    \n",
    "    return np.array(X_data), np.array(y_data)\n",
    "\n",
    "\n",
    "def train_model(X, y, model_type=\"svm\", n_features=100, save_dir=\"models\", dataset_tag=None):\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    selector = SelectKBest(score_func=f_classif, k=n_features)\n",
    "    X = selector.fit_transform(X, y)\n",
    "    \n",
    "    # model config\n",
    "    model_configs = {\n",
    "        \"svm\": {\n",
    "            'param_grid': {\n",
    "                'C': [1, 10, 20, 50, 100, 200, 500],\n",
    "                'gamma': [0.001, 0.005, 0.008, 0.015, 0.02, 0.01, 0.05, 0.1],\n",
    "                'kernel': ['rbf']\n",
    "            },\n",
    "            'base_model': SVC(probability=True, random_state=42, cache_size=1000)\n",
    "        },\n",
    "        \"svm_poly\": {\n",
    "            'param_grid': {\n",
    "                'C': [10, 50, 100, 200],\n",
    "                'gamma': [0.001, 0.005, 0.01, 0.05],\n",
    "                'kernel': ['poly'],\n",
    "                'degree': [2, 3],\n",
    "                'coef0': [0, 1]\n",
    "            },\n",
    "            'base_model': SVC(probability=True, random_state=42, cache_size=1000)\n",
    "        },\n",
    "        \"random_forest\": {\n",
    "            'param_grid': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [5, 10, 15],\n",
    "                'min_samples_split': [10, 20],\n",
    "                'min_samples_leaf': [5, 10],\n",
    "                'max_features': ['sqrt', 'log2']\n",
    "            },\n",
    "            'base_model': RandomForestClassifier(random_state=42, n_jobs=1)\n",
    "        },\n",
    "        \"logistic_regression\": {\n",
    "            'param_grid': {\n",
    "                'C': [0.1, 1, 5, 10, 50, 100],\n",
    "                'penalty': ['l2'],\n",
    "                'solver': ['lbfgs']\n",
    "            },\n",
    "            'base_model': LogisticRegression(max_iter=1000, random_state=42)\n",
    "        },\n",
    "        \"knn\": {\n",
    "            'param_grid': {\n",
    "                'n_neighbors': [3, 5, 7, 9],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'p': [1, 2]\n",
    "            },\n",
    "            'base_model': KNeighborsClassifier(n_jobs=1)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config = model_configs[model_type]\n",
    "    grid_search = GridSearchCV(\n",
    "        config['base_model'],\n",
    "        config['param_grid'],\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='accuracy',\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    print(f\"best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    cv_scores = cross_val_score(\n",
    "        grid_search.best_estimator_,\n",
    "        X,\n",
    "        y,\n",
    "        cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42),\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    print(f\"mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    \n",
    "    grid_search.best_estimator_.fit(X, y)\n",
    "    \n",
    "    paths = get_model_paths(model_type, n_features, dataset_tag=dataset_tag, base_dir=save_dir)\n",
    "    joblib.dump(grid_search.best_estimator_, paths[\"model\"])\n",
    "    joblib.dump(label_encoder, paths[\"encoder\"])\n",
    "    joblib.dump(scaler, paths[\"scaler\"])\n",
    "    joblib.dump(selector, paths[\"selector\"])\n",
    "    print(f\"model saved to {paths['model']}\")\n",
    "    \n",
    "    return grid_search.best_estimator_, label_encoder, scaler, selector\n",
    "\n",
    "def render_urdu_text(text, font_name=\"Noto Nastaliq Urdu\", font_size=48):\n",
    "    text1 = get_display(arabic_reshaper.reshape(text))\n",
    "    font = QFont(font_name, font_size)\n",
    "    image = QImage(300, 300, QImage.Format_RGB32)\n",
    "    image.fill(Qt.transparent)\n",
    "    \n",
    "    painter = QPainter(image)\n",
    "    painter.setRenderHint(QPainter.TextAntialiasing)\n",
    "    painter.setFont(font)\n",
    "    painter.setPen(Qt.green)\n",
    "    \n",
    "    fm = QFontMetrics(font)\n",
    "    x = (image.width() - fm.horizontalAdvance(text1)) // 2\n",
    "    y = (image.height() + (fm.ascent() - fm.descent())) // 2\n",
    "    painter.drawText(x, y, text1)\n",
    "    painter.end()\n",
    "    \n",
    "    ptr = image.bits()\n",
    "    ptr.setsize(image.byteCount())\n",
    "    arr = np.array(ptr).reshape(image.height(), image.width(), 4)\n",
    "    \n",
    "    return cv2.cvtColor(arr, cv2.COLOR_RGBA2BGR)\n",
    "\n",
    "\n",
    "class FingerSpellingSystem:\n",
    "    def __init__(self, conf_threshold=0.7, stability_frames=15, no_hand_frames=30):\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.stability_frames = stability_frames\n",
    "        self.no_hand_frames = no_hand_frames\n",
    "        self.reset_sentence()\n",
    "    \n",
    "    def reset_sentence(self):\n",
    "        self.sentence = \"\"\n",
    "        self.current_predictions = []\n",
    "        self.stable_prediction = None\n",
    "        self.stable_count = 0\n",
    "        self.no_hand_count = 0\n",
    "        self.last_added_letter = None\n",
    "        self.word_complete = False\n",
    "    \n",
    "    def process_prediction(self, prediction, conf):\n",
    "        self.no_hand_count = 0\n",
    "        self.word_complete = False\n",
    "        \n",
    "        if conf >= self.conf_threshold:\n",
    "            self.current_predictions.append(prediction)\n",
    "            if len(self.current_predictions) > self.stability_frames:\n",
    "                self.current_predictions.pop(0)\n",
    "            \n",
    "            if len(self.current_predictions) >= self.stability_frames:\n",
    "                common_pred, common_count = Counter(self.current_predictions).most_common(1)[0]\n",
    "                \n",
    "                if common_count / len(self.current_predictions) >= 0.7:\n",
    "                    if self.stable_prediction == common_pred:\n",
    "                        self.stable_count += 1\n",
    "                    else:\n",
    "                        self.stable_prediction = common_pred\n",
    "                        self.stable_count = 1\n",
    "                    \n",
    "                    self.stable_count = min(self.stable_count, self.stability_frames)\n",
    "                    \n",
    "                    if (self.stable_count >= self.stability_frames and \n",
    "                        self.stable_prediction != self.last_added_letter):\n",
    "                        self.sentence += self.stable_prediction\n",
    "                        self.last_added_letter = self.stable_prediction\n",
    "                        self.stable_count = 0\n",
    "        else:\n",
    "            self.current_predictions = []\n",
    "            self.stable_prediction = None\n",
    "            self.stable_count = 0\n",
    "    \n",
    "    def process_no_hand(self):\n",
    "        self.no_hand_count += 1\n",
    "        self.current_predictions = []\n",
    "        self.stable_prediction = None\n",
    "        self.stable_count = 0\n",
    "        self.last_added_letter = None\n",
    "        \n",
    "        if (self.no_hand_count >= self.no_hand_frames and \n",
    "            not self.word_complete and \n",
    "            self.sentence and \n",
    "            not self.sentence.endswith(\" \")):\n",
    "            self.sentence += \" \"\n",
    "            self.word_complete = True\n",
    "    \n",
    "    def get_sentence(self):\n",
    "        return self.sentence\n",
    "    \n",
    "    def get_status(self):\n",
    "        return {\n",
    "            'sentence': self.sentence,\n",
    "            'current_letter': self.stable_prediction or \"None\",\n",
    "            'stability': f\"{self.stable_count}/{self.stability_frames}\",\n",
    "            'confidence_needed': self.conf_threshold,\n",
    "            'predictions_buffer': len(self.current_predictions)\n",
    "        }\n",
    "\n",
    "\n",
    "def live_prediction(model, label_encoder, scaler, selector, conf_threshold=0.5):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    hands_tracking = mp_hands.Hands(static_image_mode=False,max_num_hands=1,min_detection_confidence=0.5,min_tracking_confidence=0.5)\n",
    "    \n",
    "    recent_predictions = []\n",
    "    spelling_system = FingerSpellingSystem(conf_threshold=conf_threshold,stability_frames=15,no_hand_frames=60)\n",
    "    \n",
    "    app = QApplication([])\n",
    "    window = QWidget()\n",
    "    window.setWindowTitle('Urdu Sign Language Recognition')\n",
    "    layout = QVBoxLayout()\n",
    "    label = QLabel()\n",
    "    label.setFont(QFont(\"Noto Nastaliq Urdu\", 24))\n",
    "    layout.addWidget(label)\n",
    "    window.setLayout(layout)\n",
    "    window.show()\n",
    "    \n",
    "    print(\" press 'r' to reset and 'q' to quit \")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        frame = cv2.resize(frame, (1280, 720))\n",
    "        result = hands_tracking.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        if result.multi_hand_landmarks:\n",
    "            for hand_landmarks in result.multi_hand_landmarks:\n",
    "                mp.solutions.drawing_utils.draw_landmarks(\n",
    "                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                    mp.solutions.drawing_utils.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                    mp.solutions.drawing_utils.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "                )\n",
    "                \n",
    "                landmarks = [(p.x, p.y, p.z) for p in hand_landmarks.landmark]\n",
    "                features = compute_features(landmarks)\n",
    "                features = selector.transform(scaler.transform(features.reshape(1, -1)))\n",
    "                \n",
    "                prediction = model.predict(features)[0]\n",
    "\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    conf = (np.max(model.predict_proba(features)[0]))\n",
    "                else:\n",
    "                    conf = 0.5\n",
    "                \n",
    "                recent_predictions.append(prediction)\n",
    "                if len(recent_predictions) > 5:\n",
    "                    recent_predictions.pop(0)\n",
    "                \n",
    "                label_text = label_encoder.inverse_transform([Counter(recent_predictions).most_common(1)[0][0]])[0]\n",
    "                \n",
    "                spelling_system.process_prediction(label_text, conf)\n",
    "                \n",
    "                if conf >= conf_threshold:\n",
    "                    urdu_img = render_urdu_text(label_text)\n",
    "                    h, w = urdu_img.shape[:2]\n",
    "                    if 20 + h < frame.shape[0] and 30 + w < frame.shape[1]:\n",
    "                        frame[20:20+h, 30:30+w] = urdu_img\n",
    "                    \n",
    "                    status = spelling_system.get_status()\n",
    "                    cv2.rectangle(frame, (350, 25), (1120, 60), (0, 0, 0), -1)\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        f\"Confidence: {conf:.2f} | Stable: {status['stability']}\",\n",
    "                        (350, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2\n",
    "                    )\n",
    "                else:\n",
    "                    cv2.rectangle(frame, (30, 0), (400, 80), (0, 0, 0), -1)\n",
    "                    cv2.putText(\n",
    "                        frame, \"Low confidence\",\n",
    "                        (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 2\n",
    "                    )\n",
    "        else:\n",
    "            spelling_system.process_no_hand()\n",
    "            cv2.rectangle(frame, (30, 0), (450, 80), (0, 0, 0), -1)\n",
    "            cv2.putText(\n",
    "                frame, \"No hand detected\",\n",
    "                (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 255), 2\n",
    "            )\n",
    "        \n",
    "        label.setText(spelling_system.get_sentence() or \"[Start spelling...]\")\n",
    "        cv2.imshow(\"Urdu Sign Language Recognition\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('r'):\n",
    "            spelling_system.reset_sentence()\n",
    "            label.setText(\"[Sentence reset]\")\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    app.quit()\n",
    "\n",
    "\n",
    "def model_evaluation(model_path, encoder_path, scaler_path, selector_path, test_dataset_path):\n",
    "    model = joblib.load(model_path)\n",
    "    label_encoder = joblib.load(encoder_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    selector = joblib.load(selector_path)\n",
    "    \n",
    "    X_test, y_test = load_dataset(test_dataset_path, augment=False, debug=True)\n",
    "    \n",
    "    # filter for just in case there are some classes present in test set but not in training set\n",
    "    match = [i for i, label in enumerate(y_test) if label in label_encoder.classes_]\n",
    "    X_test = X_test[match]\n",
    "    y_test = y_test[match]\n",
    "    \n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_test = selector.transform(X_test)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_, zero_division=0))\n",
    "    \n",
    "    # per-class accuracy, some labels might be broken due to compatibility issues with Urdu\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    \n",
    "    print(\"\\nPer-Class Accuracy:\")\n",
    "    for cls, acc in zip(label_encoder.classes_, per_class_acc):\n",
    "        print(f\"{cls}: {acc:.3f}\")\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.bar(label_encoder.classes_, per_class_acc, color='steelblue')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Per-Class Accuracy on Test Dataset\")\n",
    "    plt.xticks(rotation=0, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('per_class_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"plot saved to per_class_accuracy.png\")\n",
    "    \n",
    "    return accuracy, per_class_acc\n",
    "\n",
    "def feature_vector_ablation_study(model_types, feature_counts, dataset_tag):\n",
    "    res = []\n",
    "    \n",
    "    X_train, y_train = load_dataset(TRAIN_DATASET_PATH, augment=False, debug=True)\n",
    "    X_test, y_test = load_dataset(TEST_DATASET_PATH, augment=False, debug=True)\n",
    "    \n",
    "    total = len(model_types) * len(feature_counts)\n",
    "    experiment = 0\n",
    "    \n",
    "    for model_type in model_types:\n",
    "        for n_features in feature_counts:\n",
    "            experiment += 1\n",
    "            print(f\"\\n[{experiment}/{total}] {model_type.upper()} | Features: {n_features}\")\n",
    "\n",
    "            model, encoder, scaler, selector = train_model(\n",
    "                X_train, y_train,\n",
    "                model_type=model_type,\n",
    "                n_features=n_features,\n",
    "                dataset_tag=dataset_tag\n",
    "            )\n",
    "            \n",
    "            match = [i for i, label in enumerate(y_test) if label in encoder.classes_]\n",
    "            X_test = X_test[match]\n",
    "            y_test = y_test[match]\n",
    "            \n",
    "            X_test = scaler.transform(X_test)\n",
    "            X_test = selector.transform(X_test)\n",
    "            y_test = encoder.transform(y_test)\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            test_acc = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            res.append({\n",
    "                'model_type': model_type,\n",
    "                'n_features': n_features,\n",
    "                'test_accuracy': test_acc\n",
    "            })\n",
    "    \n",
    "    res_df = pd.DataFrame(res)\n",
    "    res_df.to_csv('ablation_results.csv', index=False)\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for model in model_types:\n",
    "        model_data = res_df[res_df['model_type'] == model]\n",
    "        plt.plot(model_data['n_features'], model_data['test_accuracy'], \n",
    "                marker='o', linewidth=2, label=model.upper())\n",
    "    \n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.title('Model Performance vs Feature Count')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('ablation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return res_df\n",
    "\n",
    "def individual_feature_groups(X_train, y_train, X_test, y_test, feature_groups, model_type=\"logistic_regression\"):\n",
    "    res = {}\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42, C=10)\n",
    "    \n",
    "    for group_name, indices in feature_groups.items():\n",
    "        # Use the original data for each group, not the modified version\n",
    "        scaler = StandardScaler()\n",
    "        X_train_group = scaler.fit_transform(X_train[:, indices])\n",
    "        X_test_group = scaler.transform(X_test[:, indices])\n",
    "        \n",
    "        model.fit(X_train_group, y_train)\n",
    "        \n",
    "        res[group_name] = {\n",
    "            \"n_features\": len(indices),\n",
    "            \"train_score\": model.score(X_train_group, y_train),\n",
    "            \"test_score\": model.score(X_test_group, y_test),\n",
    "            \"predictions\": model.predict(X_test_group)\n",
    "        }\n",
    "    \n",
    "    return res\n",
    "\n",
    "def cumulative_feature_groups(X_train, y_train, X_test, y_test, feature_groups, group_order):\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42, C=10)\n",
    "    res = []\n",
    "    current_indices = []\n",
    "    \n",
    "    for group_name in group_order:\n",
    "        if group_name not in feature_groups:\n",
    "            continue\n",
    "        \n",
    "        current_indices.extend(feature_groups[group_name])\n",
    "\n",
    "        # Use the original data with cumulative indices\n",
    "        scaler = StandardScaler()\n",
    "        X_train_cumulative = scaler.fit_transform(X_train[:, current_indices])\n",
    "        X_test_cumulative = scaler.transform(X_test[:, current_indices])\n",
    "        \n",
    "        model.fit(X_train_cumulative, y_train)\n",
    "        \n",
    "        train_score = model.score(X_train_cumulative, y_train)\n",
    "        test_score = model.score(X_test_cumulative, y_test)\n",
    "        \n",
    "        res.append({\n",
    "            \"group_added\": group_name,\n",
    "            \"n_features\": len(current_indices),\n",
    "            \"train_score\": train_score,\n",
    "            \"test_score\": test_score,\n",
    "            \"generalization_gap\": train_score - test_score\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "\n",
    "def feature_ablation_study():\n",
    "    X_train, y_train = load_dataset(TRAIN_DATASET_PATH, debug=True)\n",
    "    X_test, y_test = load_dataset(TEST_DATASET_PATH, debug=True)\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    \n",
    "    match = [i for i, label in enumerate(y_test) if label in label_encoder.classes_]\n",
    "    X_test = X_test[match]\n",
    "    y_test = y_test[match]\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "    \n",
    "    print(f\"\\nTrain: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    print(f\"Total features: {X_train.shape[1]}\")\n",
    "    for group, indices in FEATURE_GROUPS.items():\n",
    "        print(f\"{group}: {len(indices)} features\")\n",
    "    \n",
    "    group_results = individual_feature_groups(X_train, y_train,X_test, y_test,FEATURE_GROUPS)\n",
    "\n",
    "    group_order = [\n",
    "        'pairwise distances', 'joint angles', 'hand position size',\n",
    "        'hand orientation', 'finger spread', 'finger curvature',\n",
    "        'length ratios', 'palm relative'\n",
    "    ]\n",
    "\n",
    "    cumulative_results = cumulative_feature_groups(X_train, y_train,X_test, y_test,FEATURE_GROUPS,group_order)\n",
    "    \n",
    "    # individual feature groups graph\n",
    "    groups = list(group_results.keys())\n",
    "    train = [group_results[g][\"train_score\"] for g in groups]\n",
    "    test = [group_results[g][\"test_score\"] for g in groups]\n",
    "    gaps = [t - v for t, v in zip(train, test)]\n",
    "\n",
    "    x = np.arange(len(groups))\n",
    "    w = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    b1 = ax.bar(x - w/2, train, w, label=\"Train\", alpha=0.8)\n",
    "    b2 = ax.bar(x + w/2, test,  w, label=\"Test (External)\", alpha=0.8)\n",
    "\n",
    "    formatted_labels = [g.replace(\"_\", \"\\n\") for g in groups]\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(formatted_labels, rotation=45, ha=\"right\")\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    for bar in b1:\n",
    "        h = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, h + 0.01, f\"{h:.2f}\",ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "    \n",
    "    for bar in b2:\n",
    "        h = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, h + 0.01, f\"{h:.2f}\",ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    for i, g in enumerate(gaps):\n",
    "        top = max(train[i], test[i])\n",
    "        ax.text(x[i], top + 0.03, f\"Δ={g:.2f}\", ha=\"center\", va=\"bottom\", fontsize=9, color=\"red\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"group_performance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"figure saved to group_performance.png\")\n",
    "\n",
    "    # cumulative feature groups graph\n",
    "    x_labels = cumulative_results[\"group_added\"]\n",
    "    gains = cumulative_results[\"test_score\"].diff().fillna(cumulative_results[\"test_score\"].iloc[0])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars = ax.bar(x_labels, gains, color=\"skyblue\")\n",
    "    ax.axhline(0, linewidth=1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "    formatted_labels = [g.replace(\"_\", \"\\n\") for g in x_labels]\n",
    "    ax.set_xticklabels(formatted_labels, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Δ Test Accuracy\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, h + 0.005, f\"{h:+.3f}\",ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"marginal_gain.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"figure saved to marginal_gain.png\")\n",
    "    \n",
    "    group_df = pd.DataFrame([{'group': g, **r} for g, r in group_results.items()])\n",
    "    group_df.to_csv('feature_group_results.csv', index=False)\n",
    "    cumulative_results.to_csv('feature_cumulative_results.csv', index=False)\n",
    "    \n",
    "    return group_results, cumulative_results, FEATURE_GROUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455191f-44c9-493b-a595-cc5c44d09511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training code, specify dataset tag for model saving and loading purposes\n",
    "X, y = load_dataset(TRAIN_DATASET_PATH, augment=False, debug=True)\n",
    "model, encoder, scaler, selector = train_model(\n",
    "    X, y,\n",
    "    model_type=\"logistic_regression\",\n",
    "    n_features=100,\n",
    "    dataset_tag=DATASET_TAGS[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6190b6e2-9acc-4d61-a381-2c76466b7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation code\n",
    "paths = get_model_paths(\"logistic_regression\", 100, DATASET_TAGS[0])\n",
    "model_evaluation(\n",
    "    paths[\"model\"], paths[\"encoder\"],\n",
    "    paths[\"scaler\"], paths[\"selector\"],\n",
    "    TEST_DATASET_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f207f40-8e42-4017-9db6-bf9d279a2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running feature ablation study, loads datasets again, just in case the code is run indepently of training/testing\n",
    "group_results, cumulative_results, feature_groups = feature_ablation_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c30cf-43be-4f9f-a1a0-ea8eba4c2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running feature vector size ablation study, loads datasets again same as above\n",
    "ablation_results = feature_vector_ablation_study(\n",
    "    model_types=[\"svm\",\"svm_poly\",\"knn\",\"logistic_regression\",\"random_forest\"],\n",
    "    feature_counts=FEATURE_COUNTS,\n",
    "    dataset_tag=DATASET_TAGS[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed648da4-ccb8-41cb-bc4b-3a08585eb67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "paths = get_model_paths(model_type=\"logistic_regression\", n_features=100, dataset_tag=DATASET_TAGS[0])\n",
    "model = joblib.load(paths[\"model\"])\n",
    "encoder = joblib.load(paths[\"encoder\"])\n",
    "scaler = joblib.load(paths[\"scaler\"])\n",
    "selector = joblib.load(paths[\"selector\"])\n",
    "\n",
    "# for running real-time pipeline, for some reason causes kernel to crash upon pressing 'q'\n",
    "live_prediction(model, encoder, scaler, selector, conf_threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b94f4-3fb3-4e55-95da-b546ead7a61b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (usl_env)",
   "language": "python",
   "name": "usl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
